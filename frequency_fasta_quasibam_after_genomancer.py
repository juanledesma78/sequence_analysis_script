import subprocess as sp 
from glob import glob 
import os
import re
import argparse
import pandas as pd
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows

specs = argparse.ArgumentParser()
specs.add_argument('--dir','-d', required= True, 
                help ='path to the directory where all the genomancer results \
                have been stored. The directory should contain the files \
                quasibams.zip, post-run.zip and typonomer.zip. It returns a\
                summary of the results obtained using the pipeline genomancer\
                and consensus fasta sequences using variable frequency to call\
                the mixtures.')
args = specs.parse_args()
path_to_dir = os.path.join(os.getcwd(),args.dir)
if path_to_dir[-1] == '/':
    run_id = path_to_dir.split('/')[-2]
else:
    run_id = path_to_dir.split('/')[-1]

os.chdir(path_to_dir) 

if os.path.exists(os.path.join(path_to_dir, 'quasibam')) == False:
    quasibam = sp.run(['unzip','quasibams.zip','-d', 'quasibam'], text=True)
if os.path.exists(os.path.join(path_to_dir, 'post-run')) == False:
    postrun = sp.run(['unzip','post-run.zip','-d', 'post-run'], text=True)
if os.path.exists(os.path.join(path_to_dir, 'typonomer')) == False:
    typonomer = sp.run(['unzip','typonomer.zip','-d', 'typonomer'], text=True)


##############################################################################
# GENERATION OF DEPTH DATAFRAME AND 2/20PC FASTAS USING QUASIBAMS TABULAR FILES
##############################################################################

frequency_range = [20, 2]
dataframe_list = []
for tabular in glob('quasibam/*.tabular'):
    seq_id = (tabular.split('/')[-1]).replace('.tabular','')
    seq_id = re.sub(r'^\d+_','', seq_id)
    quasi_df = pd.read_csv(tabular , sep = "\t")
    quasi_df.rename(columns={'Depth': seq_id}, inplace = True)
    dataframe_list.append(quasi_df[seq_id])

    for frequency in frequency_range:
        outfile = os.path.join('quasibam', f'{seq_id}_{frequency}PC.fas')
        fasta_header = f'{seq_id}_{frequency}PC'
        running_quasibam_ = sp.run(["qb_post_process.pl",
                                    "-i",tabular,
                                    "-o",outfile,
                                    "-s", fasta_header,
                                    "-d", "100",
                                    "-c", str(frequency),
                                    "-n", "-"])
    tabular_output = f'quasibam/{seq_id}.tabular'
    os.rename(tabular, tabular_output)

Depth_dataframe = pd.concat(dataframe_list, axis = 1 )
Depth_dataframe.index += 1
Depth_dataframe['Position'] = Depth_dataframe.index #using the index as a column
Depth_dataframe.insert(0, 'Position', Depth_dataframe.pop('Position'))

##########################################################################
# GENERATION OF SUMMARY REPORT DATAFRAME
##########################################################################

summary_dataframe = pd.read_csv('read_counts.tsv', sep = "\t")
summary_dataframe = summary_dataframe.rename(columns={'Unnamed: 0' : 'Sample ID'})
summary_dataframe = summary_dataframe.rename(columns={'Raw' : 'Raw reads'})
summary_dataframe = summary_dataframe.rename(columns={'Trimmed' : 'Trimmed reads'})
summary_dataframe = summary_dataframe.rename(columns={'Post-prinseq' : 'Post-prinseq reads'})
summary_dataframe = summary_dataframe.rename(columns={'Dehumanised' : 'Dehumanised reads'})
summary_dataframe = summary_dataframe.rename(columns={'Mapped' : 'Mapped reads'})
summary_dataframe['gag'] = "" 
summary_dataframe['pol'] = ""
summary_dataframe['vif'] = ""
summary_dataframe['vpu'] = ""
summary_dataframe['env'] = ""
summary_dataframe['nef'] = ""
summary_dataframe['% Mapped reads'] = round((summary_dataframe['Mapped reads']*100)/summary_dataframe['Raw reads'], 2)
run_sample_list = []
for sequence in summary_dataframe['Sample ID']:
    seq_id = re.sub(r'^\d+_','', sequence)
    run_sample_list.append(seq_id)
summary_dataframe['ID'] = run_sample_list


##########################################################################
# IDENTIFYING THE FASTA SEQUENCES GENERATED BY GENOMANCER
##########################################################################

fasta_sequence_results = []
with open('genomes.fas','r') as genfas:
    fasta_file = genfas.read()
    for sample in run_sample_list:
        sample = re.sub(r'^\d+_','', sample)
        pattern = f'{sample}\.\d+'
        sequence_ids = re.findall(pattern, fasta_file)
        info =""
        for genome in sequence_ids:
                if len(info)>0:
                    info += " ; " + genome
                else:
                    info += genome
        fasta_sequence_results.append(info)

summary_dataframe['Fasta sequences'] = fasta_sequence_results

##########################################################################
# FIND THOSE SEQUENCES USED IN THE PHYLOGENY
##########################################################################

phylo_file_list = glob("post-run/*.nex") 
for phylo_file in phylo_file_list:
    with open(phylo_file, 'r') as newick:
        gene = (newick.name).replace('post-run/','').replace('.nex','')
        tree = newick.read()
       
        for n in range(len(summary_dataframe)):
            row_info = ''
            genomes = summary_dataframe.loc[n]['Fasta sequences'].split(' ; ')
            for i in range(len(genomes)):
                if genomes[i] in tree:
                    if len(row_info)>0:
                        row_info += " ; " + genomes[i]
                    else:
                        row_info += genomes[i]

            summary_dataframe.loc[n, [gene]] = row_info
                    
summary_dataframe.replace('', 'NA', inplace = True)
summary_dataframe = summary_dataframe[['ID', 'Raw reads','Trimmed reads' ,'Post-prinseq reads' ,
                    'Dehumanised reads', 'Mapped reads', '% Mapped reads','Fasta sequences',
                    'gag','pol','vif','vpu','env', 'nef' ]]


##########################################################################
# GENERATION OF EXCEL FILE CONTAINING SUMMARY AND DEPTH
##########################################################################

workbook = Workbook()
ws = workbook.active
ws1 = workbook.create_sheet(f'Summary NGS Genomancer', 0)
ws2 = workbook.create_sheet(f'Depth', 1)

for rowdf in dataframe_to_rows(summary_dataframe, index=False, header=True):
    ws1.append(rowdf)
for rowdf in dataframe_to_rows(Depth_dataframe, index=False, header=True):
    ws2.append(rowdf)

excel_file = os.path.join(path_to_dir, f'Summary_Results_{run_id}.xlsx') 
workbook.save(excel_file)
